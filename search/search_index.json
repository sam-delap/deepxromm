{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to DeepXROMM!","text":"<p>DeepXROMM integrates DeepLabCut's neural network tracking capabilities with XMAlab's XROMM (X-ray Reconstruction of Moving Morphology) workflow, enabling automated marker tracking for X-ray motion analysis.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>XMAlab Integration: Seamlessly converts between XMAlab and DeepLabCut data formats</li> <li>Flexible Tracking Modes: Choose from 2D, per-camera, or RGB merged tracking strategies</li> <li>Automated Batch Training: Train multiple projects with a single command</li> <li>Retraining Workflow: Iteratively improve networks by identifying and correcting outlier frames</li> <li>XMAlab-Style Autocorrection: Apply image processing techniques for marker refinement</li> <li>Video Analysis Tools: Compare trials and validate tracking quality</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  1. Install     \u2502\u2500\u2500&gt; Follow the Installation guide\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  2. Create      \u2502\u2500\u2500&gt; Create project and import XMAlab data\n\u2502     Project     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  3. Convert &amp;   \u2502\u2500\u2500&gt; xma_to_dlc() + create_training_dataset()\n\u2502     Prepare     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  4. Train       \u2502\u2500\u2500&gt; train_network()\n\u2502     Network     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  5. Analyze     \u2502\u2500\u2500&gt; analyze_videos() + dlc_to_xma()\n\u2502     Videos      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  6. Import to   \u2502\u2500\u2500&gt; Import predictions into XMAlab\n\u2502     XMAlab      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#detailed-steps","title":"Detailed Steps","text":"<ol> <li>Install DeepXROMM and its dependencies</li> <li>Create a project and set up your directory structure</li> <li>Import your data from XMAlab</li> <li>Train your network on tracked markers</li> <li>Analyze videos with the trained model</li> <li>Import results back into XMAlab</li> </ol>"},{"location":"#tracking-modes","title":"Tracking Modes","text":"<p>DeepXROMM supports three tracking modes to suit different experimental setups:</p> <ul> <li>2D Mode (default): Combines both camera views into a single DeepLabCut project with shared bodypart names. Both cameras train together in one network.</li> <li> <p>Best for: Standard stereo X-ray setups where both cameras view the same subject</p> </li> <li> <p>Per-Camera Mode: Creates separate DeepLabCut projects and networks for cam1 and cam2. Each camera trains independently.</p> </li> <li> <p>Best for: Different camera angles requiring specialized training, or when cameras have very different characteristics</p> </li> <li> <p>RGB Mode: Merges cam1 and cam2 into a single RGB video (cam1 \u2192 red channel, cam2 \u2192 green channel). Bodyparts are labeled with camera suffixes.</p> </li> <li>Best for: Leveraging spatial relationships between camera views, advanced training scenarios</li> </ul> <p>See the Mode parameter documentation for detailed information about each mode.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Installation Guide: Set up DeepXROMM and dependencies</li> <li>Usage Guide: Complete workflow from project creation to analysis</li> <li>Config File Reference: Detailed explanation of all configuration parameters</li> <li>Contributing Guide: Guidelines for contributing to the project</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<p>If you encounter issues or have questions: - Check the Usage Guide for detailed workflow instructions - Review the Config File Reference for parameter descriptions - Consult the Contributing Guide for development setup</p>"},{"location":"#project-layout","title":"Project Layout","text":"<p>After creating a project, your directory structure will look like this:</p> <pre><code>your-project/\n\u2502   project_config.yaml       # Main configuration file\n\u2502\n\u251c\u2500\u2500\u2500your-project-SD-YYYY-MM-DD/   # DeepLabCut project folder\n\u251c\u2500\u2500\u2500trainingdata/             # Trials with tracked markers for training\n\u2514\u2500\u2500\u2500trials/                   # Trials to analyze with trained network\n</code></pre>"},{"location":"CONTRIBUTING/","title":"Contributing Guide","text":"<p>This guide will walk you through the step-by-step how to:</p> <ol> <li>Request access as a contributor to the project</li> <li>Download the project to your local machine</li> <li>Install dependencies using uv</li> <li>Install the pre-commit hook for this repo, which will automatically enforce standard code style</li> <li>Get familiar with making changes in Git and on GitHub</li> <li>Familiarize you with how to run the test suite</li> </ol>"},{"location":"CONTRIBUTING/#request-access-to-contribute-to-the-project","title":"Request access to contribute to the project","text":"<p>This should be a fairly simple email request to Sam (sjcdelap@gmail.com, Core Maintainer) and Nicolai (nkonow@gmail.com, Lab PI) for collaboration. In this email, briefly state:</p> <ol> <li>The program you work with</li> <li>The reason you'd like to contribute to the project</li> <li>Any plans/ideas for the duration of your contributions.</li> </ol> <p>We are always on the lookout for additional maintainers, so please feel free to drop us a line!</p> <p>We will respond to you as soon as we can with next steps.</p> <p>Once you have confirmed that you have access to the repository, you can now download it onto your local machine.</p>"},{"location":"CONTRIBUTING/#download-the-project-to-your-local-machine","title":"Download the project to your local machine","text":"<p>This project, as most other collaborative code projects do, uses Git and remote code storage (via GitHub) to allow for open sharing and collaboration between groups.</p>"},{"location":"CONTRIBUTING/#install-git","title":"Install Git","text":"<p>If you haven't already, follow this guide to install git on your computer. This will let you access the code and modify it freely in a version-controllled environment</p>"},{"location":"CONTRIBUTING/#create-a-personal-access-token","title":"Create a personal access token","text":"<ol> <li>Follow GitHub's guide for creating your first personal access token. This will allow you to authenticate with GitHub whenever you download the repository.</li> </ol>"},{"location":"CONTRIBUTING/#download-clone-the-repository","title":"Download (clone) the repository","text":"<ol> <li>Navigate to the repository.</li> <li>Click the green \"code\" button above the file layout in the repostiory, then click on HTTPS</li> <li>Copy and run the command in your terminal</li> </ol>"},{"location":"CONTRIBUTING/#install-dependencies-using-uv","title":"Install dependencies using uv","text":"<p>uv is a modern, fast Python package and project manager. We are going to use it to install dependencies for this project.</p> <ol> <li>Navigate to where you had the code stored in your terminal</li> <li>Run     <pre><code>uv sync --dev --all-extras --locked\n</code></pre></li> </ol> <p>Congrats! You've now got all of the dependencies required to run deepxromm locally installed</p>"},{"location":"CONTRIBUTING/#install-the-pre-commit-hook","title":"Install the pre-commit hook","text":"<p>This step will ensure that your code meets the style guidelines for the repository.</p> <p>To install, simply run: <pre><code>uv run pre-commit install\n</code></pre></p> <p>and the styling script will install itself on your machine. You will now be alerted whenever your code does not match the style enforced by this repository.</p>"},{"location":"CONTRIBUTING/#get-familiar-with-making-changes-in-git-and-on-github","title":"Get familiar with making changes in Git and on GitHub","text":"<p>Git runs using \"branches\", which can be thought of as versions of a codebase. The public copy is called the \"main\" branch, and should only be added to after review. This guide will teach you how to make your own branch, make changes, save them in git, and push them to GitHub so that others can interact with them.</p>"},{"location":"CONTRIBUTING/#make-a-new-branch","title":"Make a new branch","text":"<p>To make a new branch (version to make a set of changes on), run: <pre><code>git checkout -b my-branch-name\n</code></pre></p> <p>This will create a new branch named <code>my-branch-name</code>. Now, make your changes</p>"},{"location":"CONTRIBUTING/#commit-your-changes-to-a-branch","title":"Commit your changes to a branch","text":"<p>When you're ready to save your changes in git, run the following script from wherever you downloaded deepxromm to: <pre><code>git add .\ngit commit -m \"My-commit-message\"\n</code></pre></p> <p>This will add all of the files you changed to the branch, and then add your changes as a new bundle with a (hopefully) descriptive message about what you changed. (\"My-commit-message\" isn't exactly all that descriptive, but you get the idea)</p>"},{"location":"CONTRIBUTING/#push-your-changes-to-github","title":"Push your changes to GitHub","text":"<p>When you're ready to push to GitHub so that others can see your code, run: <pre><code>git push\n</code></pre></p> <p>This may not work the first time that you run the command. Follow the prompts from the tool until you are prompted to enter credentials. Then enter: - Username: Your GitHub username - Password: The GitHub personal access token you made before</p>"},{"location":"CONTRIBUTING/#open-a-pull-request-in-github","title":"Open a pull request in GitHub","text":"<p>Follow this guide to create a pull request</p>"},{"location":"CONTRIBUTING/#pull-in-others-changes-from-github","title":"Pull in others' changes from GitHub","text":"<p>Others who are working on this may add more code to the main branch, or create new branches that you want to look at locally in an editor. To get their code from GitHub, simply run: <pre><code>git pull\n</code></pre></p> <p>from wherever you downloaded the project to.</p>"},{"location":"CONTRIBUTING/#familiarize-yourself-with-how-to-run-the-test-suite","title":"Familiarize yourself with how to run the test suite","text":"<p>Deepxromm provides a proactive test suite, in keeping with BDD methodology. This is to ensure that both new contributors and seasoned maintainers alike have an easy way to know if their changes have broken anything in the repository.</p> <p>The tests will run automatically whenever you open a pull request, but you can also run them manually for local testing.</p>"},{"location":"CONTRIBUTING/#running-the-full-test-suite","title":"Running the full test suite","text":"<p>Before committing any changes, always run the full test suite to ensure your changes don't break existing functionality:</p> <pre><code>DEEPXROMM_TEST_CODEC=\"XVID\" uv run pytest --cov --cov-branch --cov-report=xml\n</code></pre> <p>The <code>DEEPXROMM_TEST_CODEC</code> environment variable tells the test suite which video codec to use for testing. <code>XVID</code> is recommended for broad compatibility.</p>"},{"location":"CONTRIBUTING/#running-individual-tests-for-debugging-only","title":"Running individual tests (for debugging only)","text":"<p>If you need to debug a specific test, you can run it individually:</p> <pre><code>DEEPXROMM_TEST_CODEC=\"XVID\" uv run pytest tests/test_specific_file.py::TestClassName::test_method_name\n</code></pre> <p>\u26a0\ufe0f Important: Individual test execution is for debugging only. Always run the full test suite before committing to ensure all tests pass.</p>"},{"location":"config/","title":"DeepXROMM Config File Reference","text":""},{"location":"config/#project-settings","title":"Project Settings","text":"<p>task: The animal/behavior you\u2019re trying to study. Pulled from the name given to your project folder experimenter: Your initials working_dir: The full directory path to your project folder path_config_file: The full directory path to the DeepLabCut config for your project dataset_name: An arbitary name for your dataset. Used when generating training data for DeepLabCut, which can be found in the <code>labeled-data</code> folder inside of your DLC project folder. </p>"},{"location":"config/#neural-network-customization","title":"Neural Network Customization","text":"<p>nframes: The number of frames of video that you tracked before giving the network to DeepLabCut. Default: 0 (automatically determined from CSVs) maxiters: The maximum number of iterations to train the network for before automatically stopping training. Default: 150,000 tracking_threshold: Fraction of the total video frames to include in the training sample. Used to warn if the network detects too many/too few frames when extracting frames to be passed to the network. Default: 0.1 mode: Determines how DeepXROMM structures your training data and neural networks. Default: <code>2D</code></p> <ul> <li>2D - Combines both camera views into a single DeepLabCut project with shared bodypart names. Both cameras train together in one network.</li> <li>per_cam - Creates separate DeepLabCut projects and networks for cam1 and cam2. Each camera trains independently.</li> <li>rgb - Merges cam1 and cam2 into a single RGB video (cam1 \u2192 red channel, cam2 \u2192 green channel, blank \u2192 blue channel). Bodyparts are labeled with camera suffixes (e.g., \"marker_cam1\", \"marker_cam2\").  </li> </ul> <p>Deprecation Notice: The <code>tracking_mode</code> key is deprecated as of version 0.2.5 and will be removed in version 1.0.  Existing configs using <code>tracking_mode</code> will be automatically migrated to <code>mode</code> when loaded.  Please update your config files to use <code>mode</code> instead.</p> <p>swapped_markers: Set to \u2018true\u2019 to create artificial markers with swapped y coordinates (y coordinates of swapped-cam1 will be cam2\u2019s y coordinates). Only valid for the rgb mode crossed_markers: Set to \u2018true\u2019 to create artificial markers that are the result of multiplying the x/y positions of cam1 and cam2 together (cx_cam1_cam2_x = cam1_x * cam2_x). Only valid for the rgb mode.  </p>"},{"location":"config/#migrating-from-older-versions","title":"Migrating from Older Versions","text":""},{"location":"config/#deprecated-tracking_mode-parameter","title":"Deprecated <code>tracking_mode</code> Parameter","text":"<p>Important: The <code>tracking_mode</code> configuration key has been deprecated as of version 0.2.5 and will be removed in version 1.0.</p> <p>What changed: - Old key: <code>tracking_mode</code> - New key: <code>mode</code> - Functionality remains identical</p> <p>Automatic migration: When you load a project with the old <code>tracking_mode</code> key, DeepXROMM will automatically: 1. Read the <code>tracking_mode</code> value 2. Use it as the <code>mode</code> value 3. Log a deprecation warning</p> <p>How to update your config manually:</p> <p>Simply rename the key in your <code>project_config.yaml</code> file:</p> <p>Before (deprecated): <pre><code>tracking_mode: 2D\n</code></pre></p> <p>After (current): <pre><code>mode: 2D\n</code></pre></p> <p>Conflict detection: If your config file contains both <code>tracking_mode</code> and <code>mode</code> keys, DeepXROMM will log a warning and use the <code>mode</code> value, ignoring <code>tracking_mode</code>.</p> <p>Recommendation: Update your configuration files proactively to avoid issues when upgrading to version 1.0.</p>"},{"location":"config/#required-augmenter-settings","title":"Required Augmenter Settings","text":"<p>Important: As of recent versions, DeepXROMM requires <code>augmenter</code> settings to be present in your <code>project_config.yaml</code> file. If your configuration file was created with an older version and lacks these settings, you will encounter errors when using retraining features.</p> <p>Required configuration structure:</p> <p>Add the following to your <code>project_config.yaml</code> file if it's not already present:</p> <pre><code>augmenter:\n  outlier_algorithm: jump\n  extraction_algorithm: kmeans\n</code></pre> <p>Where to add it: Place the <code>augmenter</code> section anywhere in your <code>project_config.yaml</code> file (typically near other algorithm-related settings like <code>mode</code>).</p> <p>What these settings do: - <code>outlier_algorithm</code>: Controls how DeepXROMM detects problematic frames during retraining (default: <code>jump</code>) - <code>extraction_algorithm</code>: Controls how frames are selected for retraining (default: <code>kmeans</code>)</p> <p>See the Augmenter Settings section below for detailed information about available algorithms and when to use them.</p> <p>Note: These settings are only used during the retraining workflow (<code>extract_outlier_frames()</code> and <code>merge_datasets()</code>). If you're not using the retraining features, the default values will work fine, but the keys must still be present in your config file.</p>"},{"location":"config/#video-codec-settings","title":"Video Codec Settings","text":"<p>video_codec: Specifies the video codec used for all video operations including video conversion, RGB splitting/merging, and test video generation. Default: <code>avc1</code></p> <p>Available options:</p> <ul> <li><code>avc1</code> - H.264 codec, provides good balance of quality and compatibility</li> <li><code>XVID</code> - Xvid MPEG-4 codec</li> <li><code>DIVX</code> - DivX MPEG-4 codec</li> <li><code>mp4v</code> - MPEG-4 Part 2 codec</li> <li><code>MJPG</code> - Motion JPEG codec</li> <li><code>uncompressed</code> - No compression (very large file sizes)</li> </ul> <p>\u26a0\ufe0f Warning: Codec availability depends on your OpenCV build and operating system. DeepXROMM will raise a <code>RuntimeError</code> if the specified codec is not available on your system.</p> <p>When this setting is used:</p> <ul> <li>Creating RGB videos in <code>rgb</code> mode</li> <li>Splitting RGB videos with <code>split_rgb()</code> method</li> <li>Any video conversion operations during testing</li> <li>Frame extraction for training</li> </ul>"},{"location":"config/#video-similarity-analysis","title":"Video Similarity Analysis","text":"<p>cam1s_are_the_same_view: Controls assumptions made during project-level video similarity analysis. Default: <code>true</code></p> <p>Values: - <code>true</code> - Assumes all cam1 videos across trials capture the same view, and all cam2 videos capture the same view. Analysis compares cam1 across trials and cam2 across trials. - <code>false</code> - Assumes cam1 and cam2 capture different subjects or completely different experimental setups across trials.</p> <p>When this setting matters: - <code>analyze_video_similarity_project()</code> - Uses this setting to determine comparison strategy - <code>analyze_marker_similarity_project()</code> - Applies similar assumptions to marker trajectory analysis</p> <p>Note: This setting does NOT affect trial-level analysis methods (<code>analyze_video_similarity_trial()</code>, <code>analyze_marker_similarity_trial()</code>), which always compare cam1 vs cam2 within a single trial.</p> <p>Cross-reference: See Advanced Analysis Methods in the usage guide for detailed information about video and marker similarity analysis.</p>"},{"location":"config/#augmenter-settings","title":"Augmenter Settings","text":"<p>Controls how DeepXROMM identifies and extracts outlier frames during the retraining workflow. These settings are nested under the <code>augmenter</code> key in <code>project_config.yaml</code>. <pre><code># How the settings will appear in your project_config.yaml\naugmenter:\n    outlier_algorithm: jump\n    extraction_algorithm: kmeans\n</code></pre></p> <p>augmenter.outlier_algorithm: Algorithm used to detect outlier frames. Default: <code>jump</code></p> <p>Available algorithms:</p> <ul> <li><code>jump</code> - Detects frames with sudden jumps in predicted marker positions (recommended for most cases)</li> <li><code>fitting</code> - Identifies frames that don't fit the expected trajectory model</li> <li><code>uncertain</code> - Selects frames where the network has low confidence predictions</li> <li><code>list</code> - Use a manually specified list of frames (requires passing <code>frames2use</code> parameter to <code>extract_outlier_frames()</code>)</li> </ul> <p>augmenter.extraction_algorithm: Algorithm used to select which outlier frames to extract. Default: <code>kmeans</code></p> <p>Available algorithms:</p> <ul> <li><code>kmeans</code> - Uses k-means clustering to select diverse representative frames from outliers</li> <li><code>uniform</code> - Extracts frames uniformly distributed across the video</li> </ul> <p>When these settings are used:</p> <ul> <li><code>extract_outlier_frames()</code> - Uses both settings to identify and extract problematic frames from analyzed trials</li> </ul> <p>Cross-reference: See Retraining the Model in the usage guide for the complete retraining workflow and step-by-step instructions on using these settings.</p>"},{"location":"config/#image-processing","title":"Image Processing","text":"<p>search_area: The area, in pixels, around which autocorrect() will search for a marker. The minimum is 10, the default is 15. threshold: Grayscale value for image thresholding. Pixels with a value above this number are turned black, while pixels with a value below this number are turned white. The default is 8 (grayscale values range from 0=black to 255=white). krad: The size of the kernel used for Gaussian filtering of the image. The larger the kernel, the higher the filtered radius of a marker. The default is 17 (left) vs. a krad of 3 (right).  </p> <p>gsigma: Responsible for small differences in image contrast. Can be modified as a last resort, but for the most part I would leave this alone. The default is 10. img_wt: Relative weight of the image when it is blended together with a blur. Typically you\u2019ll want this to be significantly higher than the blur, and the default will work well for most X-ray images. The default is 3.6. blur_wt: Relative weight of the blur when it is blended together with an image. Typically you\u2019ll want this to be significantly lower than the image, and the default will work well for most X-ray images. The default is -2.9. gamma: The level of contrast in the image. Higher gamma = lower contrast. The default is 0.1 (left) vs. gamma = 0.9 (right). Try to run with a level of gamma that avoids filtering out marker data, while not taking away valuable information from the image processing itself.</p>"},{"location":"config/#autocorrect-function-visualization","title":"Autocorrect() Function Visualization","text":"<p>trial_name: The trial to use for testing cam: The camera view to use for testing frame_num: The frame to use for testing   marker: The marker to use for testing test_autocorrect: Set to \u2018true\u2019 if you want to see/troubleshoot all of the post-processing steps that autocorrect goes through for a certain trial/cam/marker/frame combination  </p> <ul> <li>Requires a way to visualize image output like Jupyter Notebook</li> <li>You can also use the provided jupyter_test_autocorrect.ipynb file from the repo</li> </ul>"},{"location":"install/","title":"Installation Guide","text":""},{"location":"install/#prerequisites","title":"Prerequisites","text":"<ol> <li>Python 3.10 (tested and known to work with DeepXROMM)</li> </ol> <p>Note: DeepXROMM depends on DeepLabCut, which requires Python 3.10+. </p>"},{"location":"install/#creating-a-conda-environment","title":"Creating a conda environment","text":"<p>Run the following command <pre><code>conda create -n your-env-name python=3.10\n</code></pre></p>"},{"location":"install/#installing-python-dependencies","title":"Installing Python dependencies","text":"<ol> <li>Activate your conda environment     <pre><code>conda activate your-env-name\n</code></pre></li> <li>If you are going to be following the tutorial in the usage guide, install this package + ipython:     <pre><code>pip install deepxromm[cli]\n</code></pre></li> <li>If you are a developer looking to install and use/extend this package in other Python scripts:     <pre><code>pip install deepxromm\n</code></pre></li> </ol> <p>Note: The <code>[cli]</code> optional dependency adds IPython for interactive use, but all core functionality works with the base package.</p>"},{"location":"usage/","title":"Usage Guide","text":"<p>There are two ways to use this package. You can either:</p> <ol> <li>Follow the usage guide below to run everything locally.</li> </ol>"},{"location":"usage/#getting-started-and-creating-a-new-project","title":"Getting started and creating a new project","text":"<ol> <li>If you haven't already, follow the steps in the installation guide to install this package!</li> <li>Activate your conda environment     <pre><code>conda activate your-env-name\n</code></pre></li> <li>Open an interactive Python session     <pre><code>ipython\n</code></pre></li> <li>From the terminal, run the following commands (replacing <code>/path/to/project-folder</code> with the path to the folder for your project and <code>SD</code> with your initials):     <pre><code>from deepxromm import DeepXROMM \nworking_dir = '/path/to/project-folder'\nexperimenter = 'SD'\n\n# Basic usage\ndeepxromm = DeepXROMM.create_new_project(working_dir, experimenter)\n\n# With custom tracking mode\ndeepxromm = DeepXROMM.create_new_project(working_dir, experimenter, mode='per_cam')\n\n# With custom video codec\ndeepxromm = DeepXROMM.create_new_project(working_dir, experimenter, codec='XVID')\n\n# With both custom mode and codec\ndeepxromm = DeepXROMM.create_new_project(working_dir, experimenter, mode='rgb', codec='DIVX')\n</code></pre></li> </ol>"},{"location":"usage/#available-tracking-modes","title":"Available tracking modes:","text":"<ul> <li><code>2D</code> (default): Combines camera data into a single DeepLabCut project</li> <li><code>per_cam</code>: Creates separate DeepLabCut projects for each camera view  </li> <li><code>rgb</code>: Blends grayscale videos into RGB channels for single-network training</li> </ul> <p>Configuration: For detailed information about each mode and related settings, see the mode parameter in the config file reference.</p>"},{"location":"usage/#video-codec-options","title":"Video codec options:","text":"<ul> <li><code>avc1</code> (default): H.264 codec, good balance of quality and compatibility</li> <li><code>XVID</code>, <code>DIVX</code>: Alternative compression codecs</li> <li><code>mp4v</code>, <code>MJPG</code>: Other supported formats</li> <li><code>uncompressed</code>: No compression (large file sizes)</li> </ul> <p>Note: Not all codecs are available on all systems. DeepXROMM will raise a <code>RuntimeError</code> if the specified codec is unavailable.</p> <p>Configuration: For more details about codec options and system compatibility, see Video Codec Settings in the config file reference.</p> <p>You should now see something that looks like this inside of your project folder:     <pre><code>sample-proj\n\u2502   project_config.yaml\n\u2502\n\u251c\u2500\u2500\u2500sample-proj-SD-YYYY-MM-DD\n\u251c\u2500\u2500\u2500trainingdata\n\u251c\u2500\u2500\u2500trials\n</code></pre></p> <p>\u26a0\ufe0f Warning: Keep your Python session open. We'll be running more commands here shortly</p>"},{"location":"usage/#exporting-your-data-from-xmalab-in-a-usable-format","title":"Exporting your data from XMAlab in a usable format","text":"<ol> <li>DeepXROMM has built-in support for training data either as:<ul> <li>Full distorted videos</li> <li>.tif/.jpg stacks (Functions are there, but untested as of mid-December 2025)</li> </ul> </li> <li>Along with your distorted videos, DeepXROMM expects CSV training data (XMAlab 2D points) exported with the following settings </li> </ol>"},{"location":"usage/#importing-your-data-and-loading-the-project","title":"Importing your data and loading the project","text":"<ol> <li>The simplest approach is to create a new folder inside of the trainingdata folder named after your trial and place your raw videos, as well as distorted 2D points from tracking, in the folder.</li> <li>There are also a number of options for customization in the project_config.yaml file. Check out the config file reference to learn more about what each variable does</li> <li>After you have added the trainingdata and/or trial folders, make sure to load the project. You should also reload it every time you update any settings.     <pre><code>deepxromm = DeepXROMM.load_project(working_dir)\n</code></pre></li> </ol>"},{"location":"usage/#converting-xmalab-data-to-deeplabcut-format","title":"Converting XMAlab data to DeepLabCut format","text":"<ol> <li>XMAlab and DeepLabCut both use CSV files (or a more data-rich format called HDF) as their primary means of storing tracking data. In order to train a network for XMAlab trials, we need to convert the XMAlab-formatted data exported in the previous step to a format that DeepLabCut can use. To do this, you can run:</li> </ol> <pre><code>deepxromm.xma_to_dlc()\n    ```\n\n## Creating a training dataset\n\n1. Next, we'll use the DLC formatted data we just extracted and codify it as our training\ndata for this run of our model. To do this, you can run:\n\n```python\ndeepxromm.create_training_dataset()\n</code></pre>"},{"location":"usage/#training-the-project","title":"Training the project","text":"<ol> <li>To start training your network, run the following in your Python terminal     <pre><code>deepxromm.train_network()\n</code></pre></li> </ol> <p>Batch processing: To train multiple projects automatically, see Batch Training Multiple Projects.</p>"},{"location":"usage/#using-a-trained-network-to-track-your-trials","title":"Using a trained network to track your trial(s)","text":"<ol> <li>Make sure any trials that you want to analyze are in appropriately named folders in the <code>trials</code> directory, and each folder contains a CSV and distorted cam1/cam2 videos that are named folder_name.csv, folder_name_cam1.avi, and folder_name_cam2.avi, respectively</li> <li>Run the following commands in your Python terminal:     <pre><code>from deepxromm import DeepXROMM\nworking_dir = '/path/to/project-folder'\ndeepxromm = DeepXROMM.load_project(working_dir)\ndeepxromm.analyze_videos()\n</code></pre></li> <li>This will save a file named trial_name-Predicted2DPoints.csv to the it# file (where number is the number next to iteration: in your project_folder/project-name-SD-YYYY-MM-DD/config.yaml file) inside of your trials/trial_name folder</li> <li>You can analyze the network's performance by importing this CSV as a 2D Points file into XMAlab with the following settings</li> </ol> <p>Quality assessment: After analysis, use the methods in Advanced Analysis Methods to validate your results.</p>"},{"location":"usage/#converting-dlc-predictions-back-to-xmalab-format","title":"Converting DLC predictions back to XMAlab format","text":"<p>After analyzing your videos with the trained network, you may want to convert the DeepLabCut predictions back to XMAlab-compatible CSV format for import into XMAlab.</p> <p>To convert predictions for all analyzed trials, run:</p> <pre><code>deepxromm.dlc_to_xma()\n</code></pre> <p>This will create <code>*-Predicted2DPoints.csv</code> files in each trial's iteration directory (e.g., <code>trials/trial_name/it0/trial_name-Predicted2DPoints.csv</code>). These files contain the network's predictions formatted with cam1/cam2 columns that XMAlab can import.</p> <p>When to use this: - After running <code>analyze_videos()</code> to get XMAlab-compatible output - When you want to visualize network predictions in XMAlab - Before importing predicted points into XMAlab for validation</p> <p>Note: This step is automatically performed when using <code>train_many_projects()</code> for batch processing.</p> <p>You can then import these CSV files into XMAlab using the same import settings shown in the Using a trained network section above.</p>"},{"location":"usage/#batch-training-multiple-projects","title":"Batch Training Multiple Projects","text":"<p>For labs with multiple DeepXROMM projects, you can automate the entire training pipeline using the <code>train_many_projects()</code> static method.</p>"},{"location":"usage/#steps-executed-during-a-batch-training-session","title":"Steps executed during a batch training session:","text":"<ol> <li>Load each project configuration</li> <li>Convert XMAlab data to DeepLabCut format (<code>xma_to_dlc()</code>)</li> <li>Create training datasets (<code>create_training_dataset()</code>)</li> <li>Train neural networks (<code>train_network()</code>)</li> <li>Analyze videos with trained models (<code>analyze_videos()</code>)</li> <li>Convert predictions back to XMAlab format (<code>dlc_to_xma()</code>)</li> </ol> <p>You can initiate a batch training session by running: <pre><code>from deepxromm import DeepXROMM\n\n# Point to parent directory containing project folders\nparent_directory = '/path/to/parent/containing/projects'\nDeepXROMM.train_many_projects(parent_directory)\n</code></pre></p>"},{"location":"usage/#formatting-your-filesystem-for-batch-training","title":"Formatting your filesystem for batch training:","text":"<ul> <li>Each subdirectory of the parent directory should be a complete DeepXROMM project</li> <li>Each project must have <code>project_config.yaml</code> </li> <li>Training data must be in <code>trainingdata/</code> folders</li> <li>Trial videos must be in <code>trials/</code> folders</li> </ul> <p>Important: This is a static method - call it directly on the <code>DeepXROMM</code> class, not on an instance.</p> <p>Cross-reference: See individual workflow steps in sections above for details on each operation performed.</p>"},{"location":"usage/#using-autocorrect","title":"Using autocorrect()","text":"<p>This package comes pre-built with autocorrect() functions that leverage the same image filtering functions as XMAlab, and use the marker's outline to do centroid detection on each marker. You can modify the autocorrect function's performance using the image processing parameters from the config file reference. You can also visualize the centroid detection process using the test_autocorrect() parameters.</p>"},{"location":"usage/#testing-autocorrect-parameters-on-a-single-markerframe-combination","title":"Testing autocorrect() parameters on a single marker/frame combination","text":"<p>You'll need a Python environment that is capable of displaying images, like a Jupyter Notebook, for these steps.</p> <ol> <li>Go to your project_config.yaml file</li> <li>Change the value of test_autocorrect to true by replacing the word \"false\" with the word \"true\", like this: <pre><code>test_autocorrect: true\n</code></pre></li> <li>Specify a trial (trial_name), camera (cam), frame number (frame_num), and marker name (marker) to test the autocorrect function on  </li> <li>Import the package and initialize a deepxromm instance as above and run the following code snippet     <pre><code>deepxromm.autocorrect_trials()\n</code></pre></li> <li>Tune autocorrect() settings until you are satisfied with the testing output</li> </ol>"},{"location":"usage/#using-autocorrect-for-a-whole-trial","title":"Using autocorrect for a whole trial","text":"<ol> <li>If you tested autocorrect, set the test_autocorrect variable in your config file to false     <pre><code>test_autocorrect: false\n</code></pre></li> <li>Import the package and initialize a deepxromm instance as a above and run the following code snippet     <pre><code>deepxromm.autocorrect_trials()\n</code></pre></li> <li>This will save a file named trial_name-AutoCorrected2DPoints.csv to the it# file (where number is the number next to iteration: in your project_folder/project-name-SD-YYYY-MM-DD/config.yaml file) inside of your trials/trial_name folder     <pre><code>iteration: 0\n</code></pre></li> <li>You can analyze autocorrect's performance by importing this CSV as a 2D Points file into XMAlab with the following settings</li> </ol>"},{"location":"usage/#retraining-the-model","title":"Retraining the Model","text":"<p>Sometimes initial training isn't sufficient for the model to generalize to new data. When predictions are poor on novel trials, you can retrain by extracting problematic frames (outliers), tracking them in XMAlab, and incorporating them into a refined training dataset.</p>"},{"location":"usage/#workflow-overview","title":"Workflow Overview","text":"<p>The retraining workflow follows these steps:</p> <ol> <li>Extract outlier frames from analyzed trials</li> <li>Track the outlier frames in XMAlab and export as CSV</li> <li>Merge the new data with your existing training dataset</li> <li>Retrain the network with the augmented dataset</li> </ol>"},{"location":"usage/#step-1-extract-outlier-frames","title":"Step 1: Extract Outlier Frames","text":"<p>After analyzing your trials, identify frames where the network performed poorly:</p> <pre><code>deepxromm.extract_outlier_frames()\n</code></pre> <p>This creates <code>outliers.yaml</code> files in each trial's iteration directory (e.g., <code>trials/trial_name/it0/outliers.yaml</code>) containing frame numbers identified as outliers.</p> <p>Advanced usage: You can pass additional parameters to customize DeepLabCut's outlier detection. These are passed directly to DeepLabCut's underlying function. See the DeepLabCut extract_outlier_frames documentation for available parameters.</p> <p>Configuration: Outlier detection behavior is controlled by settings in <code>project_config.yaml</code>. See Augmenter Settings for details.</p>"},{"location":"usage/#step-2-track-outlier-frames-in-xmalab","title":"Step 2: Track Outlier Frames in XMAlab","text":"<ol> <li>Review the frame numbers listed in <code>outliers.yaml</code></li> <li>(Optional) Edit <code>outliers.yaml</code> to include only the frames you want to track. To do this programmatically:    <pre><code>from deepxromm.project import Project\n\n# Load and edit outliers\noutliers = Project.load_config_file('trials/trial_name/it0/outliers.yaml')\noutliers = outliers[:5]  # Keep only first 5 frames\nProject.save_config_file(outliers, 'trials/trial_name/it0/outliers.yaml')\n</code></pre></li> <li>Track those frames in XMAlab for your trial</li> <li>Export the tracked data as CSV with the same settings from Exporting your data from XMAlab</li> <li>Save the CSV as <code>*outliers*.csv</code> in the iteration directory:</li> <li>Example: <code>trials/trial_name/it0/trial_name_outliers_tracking.csv</code></li> </ol> <p>Note: As long as the CSV contains the word 'outliers', all lowercase, deepxromm will detect it and use it to add new data to the training dataset Note: The CSV can contain more frames than just the outliers\u2014DeepXROMM will extract only the frames listed in <code>outliers.yaml</code>.</p>"},{"location":"usage/#step-3-merge-datasets","title":"Step 3: Merge Datasets","text":"<p>Incorporate the newly tracked outlier data into your training dataset:</p> <pre><code>deepxromm.merge_datasets()\n</code></pre> <p>This will:</p> <ul> <li>Merge outlier data with existing training data</li> <li>Increment the DeepLabCut iteration number</li> <li>Update <code>nframes</code> in your config to include the new frames</li> <li>Copy trial videos to <code>trainingdata/</code> if they weren't already there</li> </ul> <p>Parameters:</p> <ul> <li><code>update_nframes</code> (bool, default=True): Automatically update the <code>nframes</code> config value</li> <li><code>update_init_weights</code> (bool, default=True): Update initial weights to reference a previous training bout for transfer learning</li> </ul> <p>After merging, reload your project to apply the updated configuration:</p> <pre><code>deepxromm = DeepXROMM.load_project(working_dir)\n</code></pre>"},{"location":"usage/#step-4-continue-training-pipeline","title":"Step 4: Continue Training Pipeline","text":"<p>With the augmented dataset, continue through the standard training workflow:</p> <pre><code>deepxromm.xma_to_dlc()\ndeepxromm.create_training_dataset()\ndeepxromm.train_network()\ndeepxromm.analyze_videos()\ndeepxromm.dlc_to_xma()\n</code></pre> <p>The network will now train on both your original frames and the newly tracked outliers, improving performance on challenging cases.</p> <p>Cross-reference: For configuration of outlier detection algorithms, see Augmenter Settings.</p>"},{"location":"usage/#choosing-regions-with-high-variation","title":"Choosing regions with high variation","text":"<p>One thing that has been previously shown to help with neural network performance is variation of movement. To assist with finding regions of maximal dissimilarity within trial videos, we developed an automated function finds the portion of your videos with the most movement. To change how large the region we find is, simply change the size of the sliding window to suit your tracking needs.</p> <p>To use this function:</p> <ol> <li>Boot up your conda environment</li> <li>Import the package</li> <li>Load your project</li> <li>Run the following function<ol> <li>Replace /path/to/your/trial with the folder where your raw videos are stored</li> <li>Replace size_of_window with how many frames you want to track <pre><code>deepxromm.get_max_dissimilarity_for_trial('/path/to/your/trial', size_of_window)\n</code></pre></li> </ol> </li> </ol>"},{"location":"usage/#advanced-analysis-methods","title":"Advanced Analysis Methods","text":"<p>DeepXROMM provides several analysis tools to help you explore and validate your XMA project data, compare trials, and assess tracking quality.</p>"},{"location":"usage/#video-similarity-analysis","title":"Video Similarity Analysis","text":""},{"location":"usage/#project-level-video-comparison","title":"Project-Level Video Comparison","text":"<p>Analyze video similarity across all trials in your project using perceptual image hashing:</p> <pre><code>similarity_scores = deepxromm.analyze_video_similarity_project()\nprint(similarity_scores)  # Dictionary with similarity metrics\n</code></pre> <p>Returns: Dictionary with similarity scores (0 = identical videos, higher values = more different)</p> <p>Use cases: - Verify consistent camera positioning across trials - Identify outlier trials with different lighting or setup - Validate stereo camera alignment</p> <p>\u26a0\ufe0f Warning: This method assumes all cam1/cam2 pairs in your project have the same relationship. Configure the <code>cam1s_are_the_same_view</code> parameter in your project config to control this behavior. See Video Similarity Analysis in the config file reference for details.</p>"},{"location":"usage/#trial-level-video-comparison","title":"Trial-Level Video Comparison","text":"<p>Compare similarity between camera views for a specific trial:</p> <pre><code>similarity_score = deepxromm.analyze_video_similarity_trial()\nprint(f\"Cam1/Cam2 similarity: {similarity_score}\")\n</code></pre> <p>Returns: Float representing dissimilarity between the trial's camera pairs</p> <p>Use case: Validate that your stereo camera setup captures appropriately different perspectives of the same scene.</p>"},{"location":"usage/#marker-trajectory-analysis","title":"Marker Trajectory Analysis","text":""},{"location":"usage/#project-level-marker-comparison","title":"Project-Level Marker Comparison","text":"<p>Analyze marker movement patterns across all trials:</p> <pre><code>marker_similarity = deepxromm.analyze_marker_similarity_project()\nprint(marker_similarity)  # Dictionary with marker trajectory metrics\n</code></pre> <p>Returns: Dictionary with trajectory similarity scores for each marker</p> <p>Use cases: - Identify consistent vs. variable movement patterns - Detect trials with unusual marker behavior - Validate tracking consistency across experimental conditions</p> <p>\u26a0\ufe0f Warning: Like video analysis, this assumes consistent cam1/cam2 relationships across all trials. Configure the <code>cam1s_are_the_same_view</code> parameter to control this behavior. See Video Similarity Analysis in the config file reference.</p>"},{"location":"usage/#trial-level-marker-comparison","title":"Trial-Level Marker Comparison","text":"<p>Compare marker positions between camera views for a specific trial:</p> <pre><code>marker_differences = deepxromm.analyze_marker_similarity_trial()\nprint(f\"Mean position differences: {marker_differences}\")\n</code></pre> <p>Returns: Mean coordinate differences between paired markers (cam1_X - cam2_X, cam1_Y - cam2_Y)</p> <p>Use cases: - Validate marker tracking accuracy between camera views - Identify problematic markers or time periods - Assess stereo calibration quality</p>"},{"location":"usage/#utility-methods","title":"Utility Methods","text":""},{"location":"usage/#extract-marker-names-from-xmalab-data","title":"Extract Marker Names from XMAlab Data","text":"<p>Programmatically retrieve bodypart/marker names from XMAlab CSV files:</p> <pre><code>marker_names = deepxromm.get_bodyparts_from_xma(\n    csv_path='/path/to/trial/data.csv',\n    mode='2D'  # Match your project's mode (you can do this via deepxromm with deepxromm.config[\"mode\"])\n)\nprint(f\"Found markers: {marker_names}\")\n</code></pre> <p>Parameters: - <code>csv_path</code> (str): Path to XMAlab 2D points CSV file - <code>mode</code> (str): Should match your project mode (<code>'2D'</code>, <code>'per_cam'</code>, or <code>'rgb'</code>), but can be any valid mode</p> <p>Returns: List of marker names found in the CSV file</p> <p>Use cases: - Validate marker naming consistency across trials before training - Programmatically verify expected markers are present - Debug import/export issues</p> <p>Cross-reference: Use with the data import workflow described in Importing your data and loading the project.</p>"}]}