{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DeepXROMM!","text":""},{"location":"#getting-started","title":"Getting started","text":"<ol> <li>Follow the steps on our Installation page to get the package and its dependencies set up</li> <li>Follow the Usage guide to start training DeepLabCut networks for your data</li> <li>Visit our Config file reference to learn more about all of the different customization options available for training networks!</li> </ol>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>dlc-proj/        # Files generated by DeepLabCut\ntrainingdata/    # Trials for the network to train on\ntrials/          # Trials for the network to analyze\n</code></pre>"},{"location":"CONTRIBUTING/","title":"Contributing Guide","text":"<p>This guide will walk you through the step-by-step how to:</p> <ol> <li>Request access as a contributor to the project</li> <li>Download the project to your local machine</li> <li>Install dependencies using uv</li> <li>Install the pre-commit hook for this repo, which will automatically enforce standard code style</li> <li>Get familiar with making changes in Git and on GitHub</li> <li>Familiarize you with how to run the test suite</li> </ol>"},{"location":"CONTRIBUTING/#request-access-to-contribute-to-the-project","title":"Request access to contribute to the project","text":"<p>This should be a fairly simple email request to Sam (sjcdelap@gmail.com, Core Maintainer) and Nicolai (nkonow@gmail.com, Lab PI) for collaboration. In this email, briefly state:</p> <ol> <li>The program you work with</li> <li>The reason you'd like to contribute to the project</li> <li>Any plans/ideas for the duration of your contributions.</li> </ol> <p>We are always on the lookout for additional maintainers, so please feel free to drop us a line!</p> <p>We will respond to you as soon as we can with next steps.</p> <p>Once you have confirmed that you have access to the repository, you can now download it onto your local machine.</p>"},{"location":"CONTRIBUTING/#download-the-project-to-your-local-machine","title":"Download the project to your local machine","text":"<p>This project, as most other collaborative code projects do, uses Git and remote code storage (via GitHub) to allow for open sharing and collaboration between groups.</p>"},{"location":"CONTRIBUTING/#install-git","title":"Install Git","text":"<p>If you haven't already, follow this guide to install git on your computer. This will let you access the code and modify it freely in a version-controllled environment</p>"},{"location":"CONTRIBUTING/#create-a-personal-access-token","title":"Create a personal access token","text":"<ol> <li>Follow GitHub's guide for creating your first personal access token. This will allow you to authenticate with GitHub whenever you download the repository.</li> </ol>"},{"location":"CONTRIBUTING/#download-clone-the-repository","title":"Download (clone) the repository","text":"<ol> <li>Navigate to the repository.</li> <li>Click the green \"code\" button above the file layout in the repostiory, then click on HTTPS</li> <li>Copy and run the command in your terminal</li> </ol>"},{"location":"CONTRIBUTING/#install-dependencies-using-uv","title":"Install dependencies using uv","text":"<p>uv is a modern, fast Python package and project manager. We are going to use it to install dependencies for this project.</p> <ol> <li>Navigate to where you had the code stored in your terminal</li> <li>Run     <pre><code>uv sync --dev --all-extras --locked\n</code></pre></li> </ol> <p>Congrats! You've now got all of the dependencies required to run deepxromm locally installed</p>"},{"location":"CONTRIBUTING/#install-the-pre-commit-hook","title":"Install the pre-commit hook","text":"<p>This step will ensure that your code meets the style guidelines for the repository.</p> <p>To install, simply run: <pre><code>uv run pre-commit install\n</code></pre></p> <p>and the styling script will install itself on your machine. You will now be alerted whenever your code does not match the style enforced by this repository.</p>"},{"location":"CONTRIBUTING/#get-familiar-with-making-changes-in-git-and-on-github","title":"Get familiar with making changes in Git and on GitHub","text":"<p>Git runs using \"branches\", which can be thought of as versions of a codebase. The public copy is called the \"main\" branch, and should only be added to after review. This guide will teach you how to make your own branch, make changes, save them in git, and push them to GitHub so that others can interact with them.</p>"},{"location":"CONTRIBUTING/#make-a-new-branch","title":"Make a new branch","text":"<p>To make a new branch (version to make a set of changes on), run: <pre><code>git checkout -b my-branch-name\n</code></pre></p> <p>This will create a new branch named <code>my-branch-name</code>. Now, make your changes</p>"},{"location":"CONTRIBUTING/#commit-your-changes-to-a-branch","title":"Commit your changes to a branch","text":"<p>When you're ready to save your changes in git, run the following script from wherever you downloaded deepxromm to: <pre><code>git add .\ngit commit -m \"My-commit-message\"\n</code></pre></p> <p>This will add all of the files you changed to the branch, and then add your changes as a new bundle with a (hopefully) descriptive message about what you changed. (\"My-commit-message\" isn't exactly all that descriptive, but you get the idea)</p>"},{"location":"CONTRIBUTING/#push-your-changes-to-github","title":"Push your changes to GitHub","text":"<p>When you're ready to push to GitHub so that others can see your code, run: <pre><code>git push\n</code></pre></p> <p>This may not work the first time that you run the command. Follow the prompts from the tool until you are prompted to enter credentials. Then enter: - Username: Your GitHub username - Password: The GitHub personal access token you made before</p>"},{"location":"CONTRIBUTING/#open-a-pull-request-in-github","title":"Open a pull request in GitHub","text":"<p>Follow this guide to create a pull request</p>"},{"location":"CONTRIBUTING/#pull-in-others-changes-from-github","title":"Pull in others' changes from GitHub","text":"<p>Others who are working on this may add more code to the main branch, or create new branches that you want to look at locally in an editor. To get their code from GitHub, simply run: <pre><code>git pull\n</code></pre></p> <p>from wherever you downloaded the project to.</p>"},{"location":"CONTRIBUTING/#familiarize-yourself-with-how-to-run-the-test-suite","title":"Familiarize yourself with how to run the test suite","text":"<p>Deepxromm provides a proactive test suite, in keeping with BDD methodology. This is to ensure that both new contributors and seasoned maintainers alike have an easy way to know if their changes have broken anything in the repository.</p> <p>The tests will run automatically whenever you open a pull request, but you can also run them manually for local testing by running: <pre><code>uv run tests/test_deepxromm.py\n</code></pre></p>"},{"location":"config/","title":"SDLC_XMALab Config File Reference","text":""},{"location":"config/#project-settings","title":"Project Settings","text":"<p>task: The animal/behavior you\u2019re trying to study. Pulled from the name given to your project folder experimenter: Your initials working_dir: The full directory path to your project folder path_config_file: The full directory path to the DeepLabCut config for your project dataset_name: An arbitary name for your dataset. Used when generating training data for DeepLabCut, which can be found in the <code>labeled-data</code> folder inside of your DLC project folder. </p>"},{"location":"config/#neural-network-customization","title":"Neural Network Customization","text":"<p>nframes: The number of frames of video that you tracked before giving the network to DeepLabCut. Automatically determined from CSVs if set to 0 max_iters: The maximum number of iterations to train the network for before automatically stopping training. Default is 150,000 tracking_threshold: Fraction of the total video frames to include in the training sample. Used to warn if the network detects too many/too few frames when extracting frames to be passed to the network tracking_mode: Determines the mode that sdlc_xmalab will attempt to train your network with:  </p> <ul> <li>2D - cam1 and cam2 will both be passed to the network from their respective video files  </li> <li>per_cam - cam1 and cam2 have their own networks</li> <li>rgb - cam1 and cam2 will be merged into an RGB video, with cam1 as the red channel, cam2 as the green channel, and a blank frame as the blue channel  </li> </ul> <p>swapped_markers: Set to \u2018true\u2019 to create artificial markers with swapped y coordinates (y coordinates of swapped-cam1 will be cam2\u2019s y coordinates). Only valid for the rgb tracking_mode crossed_markers: Set to \u2018true\u2019 to create artificial markers that are the result of multiplying the x/y positions of cam1 and cam2 together (cx_cam1_cam2_x = cam1_x * cam2_x). Only valid for the rgb tracking_mode.  </p>"},{"location":"config/#image-processing","title":"Image Processing","text":"<p>search_area: The area, in pixels, around which autocorrect() will search for a marker. The minimum is 10, the default is 15. threshold: Grayscale value for image thresholding. Pixels with a value above this number are turned black, while pixels with a value below this number are turned white. The default is 8 (grayscale values range from 0=black to 255=white). krad: The size of the kernel used for Gaussian filtering of the image. The larger the kernel, the higher the filtered radius of a marker. The default is 17 (left) vs. a krad of 3 (right).  </p> <p>gsigma: Responsible for small differences in image contrast. Can be modified as a last resort, but for the most part I would leave this alone. The default is 10. img_wt: Relative weight of the image when it is blended together with a blur. Typically you\u2019ll want this to be significantly higher than the blur, and the default will work well for most X-ray images. The default is 3.6. blur_wt: Relative weight of the blur when it is blended together with an image. Typically you\u2019ll want this to be significantly lower than the image, and the default will work well for most X-ray images. The default is -2.9. gamma: The level of contrast in the image. Higher gamma = lower contrast. The default is 0.1 (left) vs. gamma = 0.9 (right). Try to run with a level of gamma that avoids filtering out marker data, while not taking away valuable information from the image processing itself.</p>"},{"location":"config/#autocorrect-function-visualization","title":"Autocorrect() Function Visualization","text":"<p>trial_name: The trial to use for testing cam: The camera view to use for testing frame_num: The frame to use for testing   marker: The marker to use for testing test_autocorrect: Set to \u2018true\u2019 if you want to see/troubleshoot all of the post-processing steps that autocorrect goes through for a certain trial/cam/marker/frame combination  </p> <ul> <li>Requires a way to visualize image output like Jupyter Notebook</li> <li>You can also use the provided jupyter_test_autocorrect.ipynb file from the repo</li> </ul>"},{"location":"install/","title":"Installation Guide","text":""},{"location":"install/#prerequisites","title":"Prerequisites","text":"<ol> <li>A recent version of python. These docs were built using python 3.9.16</li> <li>If you're running locally, you'll also need DeepLabCut's dependencies</li> </ol>"},{"location":"install/#creating-a-conda-environment","title":"Creating a conda environment","text":"<p>Run the following command <pre><code>conda create -n your-env-name python=your-py-version\n</code></pre></p>"},{"location":"install/#installing-python-dependencies","title":"Installing Python dependencies","text":"<ol> <li>Activate your conda environment     <pre><code>conda activate your-env-name\n</code></pre></li> <li>If you are going to be following the tutorial in the usage guide, install this package + ipython:     <pre><code>pip install deepxromm[cli]\n</code></pre></li> <li>If you are a developer looking to install and use/extend this package in other Python scripts:     <pre><code>pip install deepxromm\n</code></pre></li> </ol>"},{"location":"usage/","title":"Usage Guide","text":"<p>There are two ways to use this package. You can either:</p> <ol> <li>Follow the usage guide below to run everything locally.</li> <li>Use the colab_tutorial.ipynb Jupyter Notebook and an online computing platform like Google Colab<ol> <li>If you are using this option, be sure to make a copy of the notebook before using it so that you can save your changes!</li> </ol> </li> </ol>"},{"location":"usage/#getting-started-and-creating-a-new-project","title":"Getting started and creating a new project","text":"<ol> <li>If you haven't already, follow the steps in the installation guide to install this package!</li> <li>Activate your conda environment     <pre><code>conda activate your-env-name\n</code></pre></li> <li>Open an interactive Python session     <pre><code>ipython\n</code></pre></li> <li>From the terminal, run the following commands (replacing <code>/path/to/project-folder</code> with the path to the folder for your project and <code>SD</code> with your initials):     <pre><code>from deepxromm import DeepXROMM \nworking_dir = '/path/to/project-folder'\nexperimenter = 'SD'\ndeepxromm = DeepXROMM.create_new_project(working_dir, experimenter)\n</code></pre><ol> <li>Optionally, you can change the way your input data is fed into DeepLabCut to create one network per camera view (<code>per_cam</code>) or blend the grayscale videos into an RGB video (<code>rgb</code>) by specifying the \"mode\" parameter. For example, for per_cam: <pre><code>deepxromm = DeepXROMM.create_new_project(working_dir, experimenter, mode='per_cam')\n</code></pre></li> <li>Keep your Python session open. We'll be running more commands here shortly</li> </ol> </li> <li>You should now see something that looks like this inside of your project folder:     <pre><code>sample-proj\n\u2502   project_config.yaml\n\u2502\n\u251c\u2500\u2500\u2500sample-proj-SD-YYYY-MM-DD\n\u251c\u2500\u2500\u2500trainingdata\n\u251c\u2500\u2500\u2500trials\n</code></pre></li> </ol>"},{"location":"usage/#exporting-your-data-from-xmalab-in-a-usable-format","title":"Exporting your data from XMAlab in a usable format","text":"<ol> <li>For now, DeepXROMM only supports analyzing full distorted videos (.avi). However, we understand that many labs use distorted .tif or .jpg stacks and plan to add support for these in a later release</li> <li>Along with your distorted videos, DeepXROMM expects CSV training data (XMAlab 2D points) exported with the following settings </li> </ol>"},{"location":"usage/#importing-your-data-and-loading-the-project","title":"Importing your data and loading the project","text":"<ol> <li>The simplest approach is to create a new folder inside of the trainingdata folder named after your trial and place your raw videos, as well as distorted 2D points from tracking, in the folder.</li> <li>There are also a number of options for customization in the project_config.yaml file. Check out the config file reference to learn more about what each variable does</li> <li>After you have added the trainingdata and/or trial folders, make sure to load the project. You should also reload it every time you update any settings.     <pre><code>deepxromm = DeepXROMM.load_project(working_dir)\n</code></pre></li> </ol>"},{"location":"usage/#converting-xmalab-data-to-deeplabcut-format","title":"Converting XMAlab data to DeepLabCut format","text":"<p>XMAlab and DeepLabCut both use CSV files (or a more data-rich format called HDF) as their primary means of storing tracking data. In order to train a network for XMAlab trials, we need to convert the XMAlab-formatted data exported in the previous step to a format that DeepLabCut can use. To do this, you can run:</p> <pre><code>deepxromm.xma_to_dlc()\n</code></pre>"},{"location":"usage/#creating-a-training-dataset","title":"Creating a training dataset","text":"<p>Next, we'll use the DLC formatted data we just extracted and codify it as our training data for this run of our model. To do this, you can run:</p> <pre><code>deepxromm.create_training_dataset()\n</code></pre>"},{"location":"usage/#training-the-project","title":"Training the project","text":"<ol> <li>To start training your network, run the following in your Python terminal     <pre><code>deepxromm.train_network()\n</code></pre></li> </ol>"},{"location":"usage/#using-a-trained-network-to-track-your-trials","title":"Using a trained network to track your trial(s)","text":"<ol> <li>Make sure any trials that you want to analyze are in appropriately named folders in the <code>trials</code> directory, and each folder contains a CSV and distorted cam1/cam2 videos that are named folder_name.csv, folder_name_cam1.avi, and folder_name_cam2.avi, respectively</li> <li>Run the following commands in your Python terminal:     <pre><code>from deepxromm import DeepXROMM\nworking_dir = '/path/to/project-folder'\ndeepxromm = DeepXROMM.load_project(working_dir)\ndeepxromm.analyze_videos()\n</code></pre></li> <li>This will save a file named trial_name-Predicted2DPoints.csv to the it# file (where number is the number next to iteration: in your project_folder/project-name-SD-YYYY-MM-DD/config.yaml file) inside of your trials/trial_name folder</li> <li>You can analyze the network's performance by importing this CSV as a 2D Points file into XMAlab with the following settings</li> </ol>"},{"location":"usage/#using-autocorrect","title":"Using autocorrect()","text":"<p>This package comes pre-built with autocorrect() functions that leverage the same image filtering functions as XMAlab, and use the marker's outline to do centroid detection on each marker. You can modify the autocorrect function's performance using the image processing parameters from the config file reference. You can also visualize the centroid detection process using the test_autocorrect() parameters.</p>"},{"location":"usage/#testing-autocorrect-parameters-on-a-single-markerframe-combination","title":"Testing autocorrect() parameters on a single marker/frame combination","text":"<p>You'll need a Python environment that is capable of displaying images, like a Jupyter Notebook, for these steps  </p> <ol> <li>Go to your project_config.yaml file and find the \"Autocorrect() Testing Vars\" section of the config  </li> <li>Change the value of test_autocorrect to true by replacing the word \"false\" with the word \"true\", like this: <pre><code>test_autocorrect: true\n</code></pre></li> <li>Specify a trial (trial_name), camera (cam), frame number (frame_num), and marker name (marker) to test the autocorrect function on  </li> <li>Import the package and initialize a deepxromm instance as a above and run the following code snippet     <pre><code>deepxromm.autocorrect_trials(working_dir)\n</code></pre></li> <li>Tune autocorrect() settings until you are satisfied with the testing output</li> </ol>"},{"location":"usage/#using-autocorrect-for-a-whole-trial","title":"Using autocorrect for a whole trial","text":"<ol> <li>If you tested autocorrect, set the test_autocorrect variable in your config file to false     <pre><code>test_autocorrect: false\n</code></pre></li> <li>Import the package and initialize a deepxromm instance as a above and run the following code snippet     <pre><code>deepxromm.autocorrect_trials()\n</code></pre></li> <li>This will save a file named trial_name-AutoCorrected2DPoints.csv to the it# file (where number is the number next to iteration: in your project_folder/project-name-SD-YYYY-MM-DD/config.yaml file) inside of your trials/trial_name folder     <pre><code>iteration: 0\n</code></pre></li> <li>You can analyze autocorrect's performance by importing this CSV as a 2D Points file into XMAlab with the following settings</li> </ol>"},{"location":"usage/#choosing-regions-with-high-variation","title":"Choosing regions with high variation","text":"<p>One thing that has been previously shown to help with neural network performance is variation of movement. To assist with finding regions of maximal dissimilarity within trial videos, we developed an automated function finds the portion of your videos with the most movement. To change how large the region we find is, simply change the size of the sliding window to suit your tracking needs.</p> <p>To use this function:</p> <ol> <li>Boot up your conda environment</li> <li>Import the package</li> <li>Load your project</li> <li>Run the following function<ol> <li>Replace /path/to/your/trial with the folder where your raw videos are stored</li> <li>Replace size_of_window with how many frames you want to track <pre><code>deepxromm.get_max_dissimilarity_for_trial('/path/to/your/trial', size_of_window)\n</code></pre></li> </ol> </li> </ol>"}]}